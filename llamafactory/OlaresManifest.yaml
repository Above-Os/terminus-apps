olaresManifest.version: '0.8.1'
olaresManifest.type: app
metadata:
  name: llamafactory
  description: Easy and Efficient LLM Fine-Tuning.
  icon: https://file.bttcdn.com/appstore/radarr/icon.png
  appid: llamafactory
  version: 0.0.9
  title: LLaMA-Factory
  categories:
  - Productivity
permission:
  appData: true
  appCache: true
  userData:
  - Home
spec:
  versionName: '0.8.3'
  promoteImage:
  - https://file.bttcdn.com/appstore/radarr/promote_image_1.jpg
  - https://file.bttcdn.com/appstore/radarr/promote_image_2.jpg
  - https://file.bttcdn.com/appstore/radarr/promote_image_3.jpg
  fullDescription: |
    LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard.

    Features  
    - Various models: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Yi, Gemma, Baichuan, ChatGLM, Phi, etc.
    - Integrated methods: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.
    - Scalable resources: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.
    - Advanced algorithms: GaLore, BAdam, Adam-mini, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ, PiSSA and Agent tuning.
    - Practical tricks: FlashAttention-2, Unsloth, RoPE scaling, NEFTune and rsLoRA.
    - Experiment monitors: LlamaBoard, TensorBoard, Wandb, MLflow, etc.
    - Faster inference: OpenAI-style API, Gradio UI and CLI with vLLM worker.

  upgradeDescription: |
    # New features
    Support contamination-free packing via the neat_packing argument by @chuan298 in #4224
    Support split evaluation via the eval_dataset argument by @codemayq in #4691
    Support HQQ/EETQ quantization via the quantization_method argument by @hiyouga
    Support ZeRO-3 when using BAdam by @Ledzy in #4352
    Support train on the last turn via the mask_history argument by @aofengdaxia in #4878
    Add NPU Dockerfile by @MengqingCao in #4355
    Support building FlashAttention2 in Dockerfile by @hzhaoy in #4461
    Support batch_eval_metrics at evaluation by @hiyouga
    
    # New models
    Base models
    - InternLM2.5-7B 
    - Gemma2 (9B/27B) 
    Instruct/Chat models
    - TeleChat-1B-Chat by @hzhaoy in #4651
    - InternLM2.5-7B-Chat
    - CodeGeeX4-9B-Chat 
    - Gemma2-it (9B/27B)
    
    # Changes
    Fix DPO cutoff len and deprecate reserved_label_len argument
    Improve loss function for reward modeling

    And many bug fix

    Full release note at: https://github.com/hiyouga/LLaMA-Factory/releases/tag/v0.8.3

  developer: hiyouga
  website: https://github.com/hiyouga/LLaMA-Factory
  sourceCode: https://github.com/hiyouga/LLaMA-Factory
  submitter: Terminus
  language:
  - en
  doc: https://llamafactory.readthedocs.io/zh-cn/latest/
  license:
  - text: Apache-2.0
    url: https://github.com/hiyouga/LLaMA-Factory?tab=Apache-2.0-1-ov-file#readme
  requiredMemory: 64Mi
  limitedMemory: 256Mi
  requiredDisk: 128Mi
  limitedDisk: 256Mi
  requiredCpu: 0.1
  limitedCpu: 0.5
  supportArch:
  - arm64
  - amd64
options:
  dependencies:
  - name: olares
    type: system
    version: '>=1.10.0-0'
entrances:
- name: radarr-svc
  port: 7878
  host: radarr-svc
  title: Radarr
  icon: https://file.bttcdn.com/appstore/radarr/icon.png
