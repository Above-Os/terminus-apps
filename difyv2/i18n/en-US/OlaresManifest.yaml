metadata:
  title: Dify Shared
  description: The Innovation Engine for GenAI Applications
spec:
  fullDescription: |
    ## IMPORTANT NOTE ##
    This is a shared version of Dify. Only one service will be installed across the entire Olares system. However, every user still needs to install an desktop entry to access and use the service installed by the Olares Admin.

    After installation, everyone on this Olares cluster can use the same application. This shared setup makes it easier for users to share information with each other, while reducing maintenance work and using fewer resources.

    ## Migration Note ##
    If you're using an older version of Dify, it might conflict with Dify v1.0 or later. We recommend backing up your data, uninstalling the old version, and then installing this latest version.

    ## OVERVIEW ##
    Dify is an open-source LLM app development platform. Its intuitive interface combines AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production. Here's a list of the core features:

    1. Workflow: Build and test powerful AI workflows on a visual canvas, leveraging all the following features and beyond.

    2. Comprehensive model support: Seamless integration with hundreds of proprietary / open-source LLMs from dozens of inference providers and self-hosted solutions, covering GPT, Mistral, Llama3, and any OpenAI API-compatible models. A full list of supported model providers can be found here.

    3. Prompt IDE: Intuitive interface for crafting prompts, comparing model performance, and adding additional features such as text-to-speech to a chat-based app.

    4. RAG Pipeline: Extensive RAG capabilities that cover everything from document ingestion to retrieval, with out-of-box support for text extraction from PDFs, PPTs, and other common document formats.

    5. Agent capabilities: You can define agents based on LLM Function Calling or ReAct, and add pre-built or custom tools for the agent. Dify provides 50+ built-in tools for AI agents, such as Google Search, DELL·E, Stable Diffusion and WolframAlpha.

    6. LLMOps: Monitor and analyze application logs and performance over time. You could continuously improve prompts, datasets, and models based on production data and annotations.

    7. Backend-as-a-Service: All of Dify's offerings come with corresponding APIs, so you could effortlessly integrate Dify into your own business logic.
  upgradeDescription: |
    Upgrade to Dify v1.13.0 - Human-in-the-Loop and Workflow Execution Upgrades

    ## New Features

    ### Human-in-the-Loop (HITL)

    We are introducing the **Human Input** node, a major update that transforms how AI and humans collaborate within Dify workflows.

    **Key Capabilities:**

    - **Native Workflow Pausing:** Insert a "Human Input" node to suspend workflow execution at critical decision points.
    - **Review & Edit:** The node generates a UI where humans can review AI outputs and modify variables (e.g., editing a draft or correcting data) before the process continues.
    - **Action-Based Routing:** Configure custom buttons (like "Approve," "Reject," or "Escalate") that determine the subsequent path of the workflow.
    - **Flexible Delivery Methods:** Human input forms can be delivered via **Webapp** or **Email**. In cloud environments, Email delivery availability may depend on plan/feature settings.

    ## Architecture Updates

    To support the stateful pause/resume mechanism required by HITL and provide event‑subscription APIs, we refactored the execution engine: **Workflow‑based streaming executions and Advanced Chat executions now run in Celery workers**, while non‑streaming **WORKFLOW** runs still execute in the API process. All pause/resume paths (e.g., HITL) are resumed via Celery, and events are streamed back through Redis Pub/Sub.

    **For Large Deployments & Self-Hosted Users:**

    We have introduced a new Celery queue named `workflow_based_app_execution`. While standard setups will work out of the box, high-throughput environments should consider the following optimizations:

    1. **Scale Workers:** Adjust the number of workers consuming the `workflow_based_app_execution` queue based on your specific workload.
    2. **Dedicated Redis (Optional):** For large-scale deployments, we recommend configuring the new `PUBSUB_REDIS_URL` environment variable to point to a dedicated Redis instance. Using **Redis Cluster mode with Sharded PubSub** is strongly advised to ensure horizontal scalability.

    ## Important Upgrade Note

    **New Celery Queue Required: `workflow_based_app_execution`**

    Please ensure your deployment configuration includes workers listening to the new **`workflow_based_app_execution`** queue. This queue is required for workflow‑based streaming executions and all resume flows (e.g., HITL); otherwise, streaming executions and resume tasks will not be processed.

    ## Additional Updates

    - Enhanced workflow run history management and UI updates
    - Performance optimizations for API endpoints and database operations
    - Multiple bug fixes and improvements across the platform
    - Support for nl-NL language
    
    View full release note at: https://github.com/langgenius/dify/releases/tag/1.13.0