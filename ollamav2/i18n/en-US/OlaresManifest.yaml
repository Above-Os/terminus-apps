metadata:
  title: Ollama
  description: Get up and running with large language models.

spec:
  fullDescription: |
    ## IMPORTANT NOTE ##
    This is a shared app. Once installed by the Olares Admin, all users in the cluster can use it through reference app.

    ## OVERVIEW ##
    Ollama is a user-friendly interface for running large language models (LLMs) locally. It is a valuable tool for researchers, developers, and anyone who wants to experiment with language models. With Ollama, you can easily download and run LLMs, customize and create your own, and chat with your LLMs using files on your device.

    Ollama supports a wide range of models, including:
    - Llama 3 8B
    - Llama 3 70B
    - Phi 3 Mini 3.8B
    - Phi 3 Medium 14B
    - Gemma 2 9B
    - Gemma 2 27B
    - Mistral 7B
    - Moondream 2 1.4B
    - Neural Chat 7B
    - Starling 7B
    - Code Llama 7B
    - Llama 2 Uncensored 7B
    - LLaVA 7B
    - Solar 10.7B

  upgradeDescription: |
    Upgrade Ollama version to v0.13.1

    # New models
    - Ministral-3: The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.
    - Mistral-Large-3: A general-purpose multimodal mixture-of-experts model for production-grade tasks and enterprise workloads.

    # What's Changed
    - nomic-embed-text will now use Ollama's engine by default
    - Tool calling support for cogito-v2.1
    - Fixed issues with CUDA VRAM discovery
    - Fixed link to docs in Ollama's app
    - Fixed issue where models would be evicted on CPU-only systems
    - Ollama will now better render errors instead of showing Unmarshal: errors
    - Fixed issue where CUDA GPUs would fail to be detected with older GPUs
    - Added thinking and tool parsing for cogito-v2.1


    Full release notes:
    https://github.com/ollama/ollama/releases/tag/v0.13.1
