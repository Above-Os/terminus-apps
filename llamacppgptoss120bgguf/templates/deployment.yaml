{{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
{{- $llamaDomainENV := split "," .Values.domain.llamaclient -}}
{{- $llamaDomain := index $llamaDomainENV "_0"  -}}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama
  namespace: "{{ .Release.Namespace }}"
  labels:
    io.kompose.service: llama
  annotations:
    applications.app.bytetrade.io/gpu-inject: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: llama
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        io.kompose.network/chrome-default: "true"
        io.kompose.service: llama
    spec:
      restartPolicy: Always
      containers:
        - name: llama
          image: aboveos/ggml-org-llama.cpp:full-cuda-b6823
          command:
            - /bin/sh
            - -c
            - |
              DONE_FILE="/models/.gpt-oss-120b-GGUF.done"
              echo "[wait] Waiting for $DONE_FILE ..."
              until [ -f "$DONE_FILE" ]; do
                sleep 2
              done
              echo "[wait] Detected $DONE_FILE, starting server."
              exec ./llama-server "$@"
          args:
            - "--server"
            - "-m"
            - "/models/gpt-oss-120b-mxfp4-00001-of-00003.gguf"
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8080"
            - "-c"
            - "8000"
            - "--flash-attn"
            - "auto"
            - "--jinja"
            - "--reasoning-format"
            - "none"
            - "--n-gpu-layers"
            - "60"
            - "--n-cpu-moe"
            - "24"
            - "--temp"
            - "1.0"
            - "--top-p"
            - "1.0"
            - "--top-k"
            - "0"
            - "--threads"
            - "20"
          env:
            - name: PGID
              value: "1000"
            - name: PUID
              value: "1000"
            - name: TZ
              value: "Etc/UTC"
            - name: LLAMA_CACHE
              value: "/models/cache"
            - name: HF_HOME
              value: "/models/.cache/huggingface"
            - name: HUGGINGFACE_HUB_CACHE
              value: "/models/hub"
          ports:
            - containerPort: 8080
              name: http
          resources:
            limits:
              cpu: "14"
              memory: 69Gi
            requests:
              cpu: "6"
              memory: 9Gi
          volumeMounts:
            - name: data
              mountPath: /models
      volumes:
        - name: data
          hostPath:
            path: "{{ .Values.userspace.userData }}/Huggingface/gptoss120bgguf"
            type: DirectoryOrCreate

---
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Release.Name }}-download-models
  namespace: {{ .Release.Namespace }}
  labels:
    app: {{ .Release.Name }}-download-models
spec:
  ttlSecondsAfterFinished: 100
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: {{ .Release.Name }}-download-models
    spec:
      restartPolicy: OnFailure
      containers:
        - name: download-model
          image: harveyff/hf-downloader:v0.0.5
          env:
            - name: HF_ENDPOINT
              value: "{{ .Values.olaresEnv.OLARES_USER_HUGGINGFACE_SERVICE }}"
            - name: HF_TOKEN
              value: "{{ .Values.olaresEnv.OLARES_USER_HUGGINGFACE_TOKEN }}"
          args:
            - "--repo"
            - "ggml-org/gpt-oss-120b-GGUF"
            - "--ref"
            - "main"
            - "--outdir"
            - "/data"
            - "--static"
            - "/app/static"
            - "--port"
            - "8090"
            - "--probe-url"
            - "https://{{ $llamaDomain }}/backend_health"
            - "--done-name"
            - ".gpt-oss-120b-GGUF.done"
          ports:
            - containerPort: 8090
              name: http
          volumeMounts:
            - name: data
              mountPath: /data
          resources:
            requests:
              cpu: "500m"
              memory: "500Mi"
            limits:
              cpu: "1"
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
      volumes:
        - name: data
          hostPath:
            path: "{{ .Values.userspace.userData }}/Huggingface/gptoss120bgguf"
            type: DirectoryOrCreate
---
apiVersion: v1
kind: Service
metadata:
  name: download-svc
  namespace: {{ .Release.Namespace }}
spec:
  selector:
    app: {{ .Release.Name }}-download-models
  ports:
    - name: download-status
      port: 8090
      targetPort: 8090
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: llama
  namespace: "{{ .Release.Namespace }}"
  labels:
    io.kompose.service: llama
spec:
  selector:
    io.kompose.service: llama
  ports:
    - name: http
      port: 8080
      targetPort: 8080
  type: ClusterIP
{{- end }}
