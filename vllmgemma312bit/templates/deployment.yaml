{{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
{{- $vllmDomainENV := split "," .Values.domain.vllmclient -}}
{{- $vllmDomain := index $vllmDomainENV "_0" -}}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  name: vllm
  namespace: "{{ .Release.Namespace }}"
  labels:
    io.kompose.service: vllm
  annotations:
    applications.app.bytetrade.io/gpu-inject: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: vllm
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        io.kompose.network/chrome-default: "true"
        io.kompose.service: vllm
    spec:
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.11.0
          command:
            - /bin/bash
            - -c
            - |
              DONE_FILE="/models/gemma-3-12b/.gemma-3-12b-it-quantized-W4A16.done"
              echo "[vllm-wait] Waiting for $DONE_FILE ..."
              while [ ! -f "$DONE_FILE" ]; do
                echo "[vllm-wait] Model not ready, sleeping..."
                sleep 10
              done
              echo "[vllm-wait] Detected $DONE_FILE, starting vLLM server."
              
              echo "[vllm-start] Starting vLLM server..."
              python3 -m vllm.entrypoints.openai.api_server \
                --model "/models/gemma-3-12b" \
                --max-model-len 8000 \
                --tensor-parallel-size 1
          env:
            - name: PGID
              value: "1000"
            - name: PUID
              value: "1000"
            - name: TZ
              value: Etc/UTC
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: VLLM_PORT
              value: "8000"
          ports:
            - containerPort: 8000
          startupProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            timeoutSeconds: 60
            periodSeconds: 40
            failureThreshold: 120
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            timeoutSeconds: 60
            periodSeconds: 60
            successThreshold: 1
            failureThreshold: 10
          resources:
            limits:
              cpu: "18"
              memory: 18Gi
            requests:
              cpu: 500m
              memory: 9Gi
          volumeMounts:
            - mountPath: /models/gemma-3-12b
              name: data
      volumes:
        - name: data
          hostPath:
            path: "{{ .Values.userspace.userData }}/Huggingface/gemma312bit"
            type: DirectoryOrCreate
      restartPolicy: Always
status: {}

---
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Release.Name }}-download-models
  namespace: {{ .Release.Namespace }}
  labels:
    app: {{ .Release.Name }}-download-models
spec:
  ttlSecondsAfterFinished: 100
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: {{ .Release.Name }}-download-models
    spec:
      restartPolicy: OnFailure
      containers:
        - name: download-model
          image: harveyff/hf-downloader:v0.0.5
          env:
            - name: HF_ENDPOINT
              value: "{{ .Values.olaresEnv.OLARES_USER_HUGGINGFACE_SERVICE }}"
            - name: HF_TOKEN
              value: "{{ .Values.olaresEnv.OLARES_USER_HUGGINGFACE_TOKEN }}"
          args:
            - "--repo"
            - "abhishekchohan/gemma-3-12b-it-quantized-W4A16"
            - "--ref"
            - "main"
            - "--outdir"
            - "/data"
            - "--static"
            - "/app/static"
            - "--port"
            - "8090"
            - "--probe-url"
            - "https://{{ $vllmDomain }}/backend_health"
            - "--done-name"
            - ".gemma-3-12b-it-quantized-W4A16.done"
          ports:
            - containerPort: 8090
              name: http
          volumeMounts:
            - name: data
              mountPath: /data
          resources:
            requests:
              cpu: "100m"
              memory: "500Mi"
            limits:
              cpu: "1"
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
      volumes:
        - name: data
          hostPath:
            path: "{{ .Values.userspace.userData }}/Huggingface/gemma312bit"
            type: DirectoryOrCreate
---
apiVersion: v1
kind: Service
metadata:
  name: download-svc
  namespace: {{ .Release.Namespace }}
spec:
  selector:
    app: {{ .Release.Name }}-download-models
  ports:
    - name: download-status
      port: 8090
      targetPort: 8090
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  name: vllm
  namespace: "{{ .Release.Namespace }}"
  labels:
    io.kompose.service: vllm
spec:
  ports:
    - name: http
      port: 8000
      targetPort: 8000
  selector:
    io.kompose.service: vllm
status:
  loadBalancer: {}
{{- end }}