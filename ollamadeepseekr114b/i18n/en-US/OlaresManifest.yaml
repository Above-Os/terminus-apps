metadata:
  title: DeepSeek-R1 14B (Ollama)
  description: 一系列开放推理模型，其性能接近领先模型。

spec:
  fullDescription: |
    ## IMPORTANT NOTE ##
    This is a shared app. Once installed by the Olares Admin, all users in the cluster can use it through reference app.

    ## MODEL OVERVIEW ##
    DeepSeek-R1 has received a minor version upgrade to DeepSeek-R1-0528 for the 8 billion parameter distilled model and the full 671 billion parameter model. In this update, DeepSeek R1 has significantly improved its reasoning and inference capabilities. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.

    # DeepSeek-R1 Models
    DeepSeek-R1-Zero
    - Total Params:671B
    - Activated Params: 37B
    - Context Length: 128K

    DeepSeek-R1
    - Total Params:671B
    - Activated Params: 37B
    - Context Length: 128K

    DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. For more details regarding the model architecture, please refer to DeepSeek-V3 repository.

    # DeepSeek-R1-Distill Models
    DeepSeek-R1-Distill-Qwen-1.5B
    - Base Model: Qwen2.5-Math-1.5B

    DeepSeek-R1-Distill-Qwen-7B
    - Base Model: Qwen2.5-Math-7B

    DeepSeek-R1-Distill-Llama-8B
    - Base Model: Llama-3.1-8B

    DeepSeek-R1-Distill-Qwen-14B
    - Base Model: Qwen2.5-14B

    DeepSeek-R1-Distill-Qwen-32B
    - Base Model: Qwen2.5-32B

    DeepSeek-R1-Distill-Llama-70B
    - Base Model: Llama-3.3-70B-Instruct

    DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1. We slightly change their configs and tokenizers. Please use our setting to run these models.
